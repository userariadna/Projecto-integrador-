{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsaNNMPuLl21",
        "outputId": "f4a9c41a-40e4-4509-e0e8-1447647a45fa"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#Import file\n",
        "with io.open(\"positivo.csv\", encoding='utf-8') as f: #\n",
        "    text = f.read().lower() #read file applying lower cases\n",
        "print('corpus length:', len(text))\n",
        "#print length of corpus and the \n",
        "chars = sorted(list(set(text))) \n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars)) #To turn letters into numbers\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars)) #To turn numbers into letters\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length: 36314\n",
            "total chars: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3auf0mGYK210"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('chars.pickle', 'wb') as f:\n",
        "  pickle.dump(chars, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRm1IqmiLWA3"
      },
      "source": [
        "with open('char_indices.pickle', 'wb') as f:\n",
        "  pickle.dump(char_indices, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om_FxlkyLWHG"
      },
      "source": [
        "with open('indices_char.pickle', 'wb') as f:\n",
        "  pickle.dump(indices_char, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiMRxltc_nl8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ09PyZfLogL",
        "outputId": "c82c5b57-f91e-4e11-deeb-4ba781045dfa"
      },
      "source": [
        "maxlen = 40 #Sequence of length\n",
        "step = 3 #Size of step\n",
        "sentences = [] #Holding all of the split up sentences \n",
        "next_chars = [] #Holding the next letter in the sequence\n",
        "\n",
        "#Encoding\n",
        "for i in range(0, len(text) - maxlen, step): \n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "#Make it processable by the neural network\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "#Model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "#Function to help sampling an index from a probability array\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "#Function called at the end of each epoch which prints a generated text\n",
        "def on_epoch_end(epoch, _):\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb sequences: 12092\n",
            "Vectorization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy9LR5iXXaD7",
        "outputId": "6e6cbd8d-be03-4363-e441-7e5ac7aa390b"
      },
      "source": [
        "verbose = 1 #progress bar\n",
        "def train_model(model, X, y, batch_size=128, nb_epoch=100, verbose=0):#Function to train the neural network model\n",
        "    checkpointer = ModelCheckpoint(filepath=\"weights_E.hdf5\", monitor='loss', verbose=verbose, save_best_only=True, mode='min') #Saving the model weights\n",
        "    model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, verbose=verbose, callbacks=[checkpointer])\n",
        "    model.save('GenerativeModel_compiled') #Generative model deployment\n",
        "train_model(model, x, y, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 3.0574\n",
            "Epoch 00001: loss improved from inf to 3.05741, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 31s 280ms/step - loss: 3.0574\n",
            "Epoch 2/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 2.5227\n",
            "Epoch 00002: loss improved from 3.05741 to 2.52274, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 2.5227\n",
            "Epoch 3/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 2.3067\n",
            "Epoch 00003: loss improved from 2.52274 to 2.30668, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 2.3067\n",
            "Epoch 4/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 2.1582\n",
            "Epoch 00004: loss improved from 2.30668 to 2.15821, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 2.1582\n",
            "Epoch 5/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 2.0363\n",
            "Epoch 00005: loss improved from 2.15821 to 2.03633, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 2.0363\n",
            "Epoch 6/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.9263\n",
            "Epoch 00006: loss improved from 2.03633 to 1.92632, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 1.9263\n",
            "Epoch 7/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.8180\n",
            "Epoch 00007: loss improved from 1.92632 to 1.81798, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 1.8180\n",
            "Epoch 8/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.7111\n",
            "Epoch 00008: loss improved from 1.81798 to 1.71111, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 1.7111\n",
            "Epoch 9/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.6068\n",
            "Epoch 00009: loss improved from 1.71111 to 1.60681, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 1.6068\n",
            "Epoch 10/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.5274\n",
            "Epoch 00010: loss improved from 1.60681 to 1.52742, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 1.5274\n",
            "Epoch 11/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.4485\n",
            "Epoch 00011: loss improved from 1.52742 to 1.44846, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 1.4485\n",
            "Epoch 12/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.3676\n",
            "Epoch 00012: loss improved from 1.44846 to 1.36762, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 1.3676\n",
            "Epoch 13/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.2982\n",
            "Epoch 00013: loss improved from 1.36762 to 1.29823, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 1.2982\n",
            "Epoch 14/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.2302\n",
            "Epoch 00014: loss improved from 1.29823 to 1.23022, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 1.2302\n",
            "Epoch 15/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.1925\n",
            "Epoch 00015: loss improved from 1.23022 to 1.19253, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 1.1925\n",
            "Epoch 16/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.1381\n",
            "Epoch 00016: loss improved from 1.19253 to 1.13811, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 1.1381\n",
            "Epoch 17/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.0933\n",
            "Epoch 00017: loss improved from 1.13811 to 1.09329, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 1.0933\n",
            "Epoch 18/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.0541\n",
            "Epoch 00018: loss improved from 1.09329 to 1.05407, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 1.0541\n",
            "Epoch 19/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.0244\n",
            "Epoch 00019: loss improved from 1.05407 to 1.02437, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 1.0244\n",
            "Epoch 20/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.9953\n",
            "Epoch 00020: loss improved from 1.02437 to 0.99535, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.9953\n",
            "Epoch 21/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.9501\n",
            "Epoch 00021: loss improved from 0.99535 to 0.95012, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.9501\n",
            "Epoch 22/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.9397\n",
            "Epoch 00022: loss improved from 0.95012 to 0.93967, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.9397\n",
            "Epoch 23/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.9003\n",
            "Epoch 00023: loss improved from 0.93967 to 0.90025, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.9003\n",
            "Epoch 24/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8991\n",
            "Epoch 00024: loss improved from 0.90025 to 0.89907, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 283ms/step - loss: 0.8991\n",
            "Epoch 25/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8710\n",
            "Epoch 00025: loss improved from 0.89907 to 0.87096, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.8710\n",
            "Epoch 26/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8538\n",
            "Epoch 00026: loss improved from 0.87096 to 0.85376, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 0.8538\n",
            "Epoch 27/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8449\n",
            "Epoch 00027: loss improved from 0.85376 to 0.84494, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.8449\n",
            "Epoch 28/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8295\n",
            "Epoch 00028: loss improved from 0.84494 to 0.82954, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 282ms/step - loss: 0.8295\n",
            "Epoch 29/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8027\n",
            "Epoch 00029: loss improved from 0.82954 to 0.80268, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.8027\n",
            "Epoch 30/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.8125\n",
            "Epoch 00030: loss did not improve from 0.80268\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.8125\n",
            "Epoch 31/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7756\n",
            "Epoch 00031: loss improved from 0.80268 to 0.77557, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 0.7756\n",
            "Epoch 32/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7738\n",
            "Epoch 00032: loss improved from 0.77557 to 0.77379, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.7738\n",
            "Epoch 33/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7535\n",
            "Epoch 00033: loss improved from 0.77379 to 0.75345, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.7535\n",
            "Epoch 34/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7514\n",
            "Epoch 00034: loss improved from 0.75345 to 0.75137, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.7514\n",
            "Epoch 35/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7458\n",
            "Epoch 00035: loss improved from 0.75137 to 0.74583, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.7458\n",
            "Epoch 36/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7297\n",
            "Epoch 00036: loss improved from 0.74583 to 0.72974, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.7297\n",
            "Epoch 37/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7079\n",
            "Epoch 00037: loss improved from 0.72974 to 0.70787, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.7079\n",
            "Epoch 38/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7260\n",
            "Epoch 00038: loss did not improve from 0.70787\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.7260\n",
            "Epoch 39/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7137\n",
            "Epoch 00039: loss did not improve from 0.70787\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.7137\n",
            "Epoch 40/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.7046\n",
            "Epoch 00040: loss improved from 0.70787 to 0.70457, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.7046\n",
            "Epoch 41/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6876\n",
            "Epoch 00041: loss improved from 0.70457 to 0.68762, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.6876\n",
            "Epoch 42/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6878\n",
            "Epoch 00042: loss did not improve from 0.68762\n",
            "95/95 [==============================] - 27s 283ms/step - loss: 0.6878\n",
            "Epoch 43/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6805\n",
            "Epoch 00043: loss improved from 0.68762 to 0.68050, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.6805\n",
            "Epoch 44/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6702\n",
            "Epoch 00044: loss improved from 0.68050 to 0.67016, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.6702\n",
            "Epoch 45/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6552\n",
            "Epoch 00045: loss improved from 0.67016 to 0.65521, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.6552\n",
            "Epoch 46/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6479\n",
            "Epoch 00046: loss improved from 0.65521 to 0.64790, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 0.6479\n",
            "Epoch 47/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6466\n",
            "Epoch 00047: loss improved from 0.64790 to 0.64660, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.6466\n",
            "Epoch 48/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6406\n",
            "Epoch 00048: loss improved from 0.64660 to 0.64057, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.6406\n",
            "Epoch 49/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6445\n",
            "Epoch 00049: loss did not improve from 0.64057\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.6445\n",
            "Epoch 50/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6136\n",
            "Epoch 00050: loss improved from 0.64057 to 0.61362, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.6136\n",
            "Epoch 51/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6259\n",
            "Epoch 00051: loss did not improve from 0.61362\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.6259\n",
            "Epoch 52/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.6126\n",
            "Epoch 00052: loss improved from 0.61362 to 0.61258, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 0.6126\n",
            "Epoch 53/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5984\n",
            "Epoch 00053: loss improved from 0.61258 to 0.59839, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.5984\n",
            "Epoch 54/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5859\n",
            "Epoch 00054: loss improved from 0.59839 to 0.58586, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 284ms/step - loss: 0.5859\n",
            "Epoch 55/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5852\n",
            "Epoch 00055: loss improved from 0.58586 to 0.58518, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 0.5852\n",
            "Epoch 56/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5729\n",
            "Epoch 00056: loss improved from 0.58518 to 0.57288, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 282ms/step - loss: 0.5729\n",
            "Epoch 57/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5970\n",
            "Epoch 00057: loss did not improve from 0.57288\n",
            "95/95 [==============================] - 27s 282ms/step - loss: 0.5970\n",
            "Epoch 58/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5855\n",
            "Epoch 00058: loss did not improve from 0.57288\n",
            "95/95 [==============================] - 27s 282ms/step - loss: 0.5855\n",
            "Epoch 59/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5685\n",
            "Epoch 00059: loss improved from 0.57288 to 0.56850, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 0.5685\n",
            "Epoch 60/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5730\n",
            "Epoch 00060: loss did not improve from 0.56850\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5730\n",
            "Epoch 61/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5713\n",
            "Epoch 00061: loss did not improve from 0.56850\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.5713\n",
            "Epoch 62/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5479\n",
            "Epoch 00062: loss improved from 0.56850 to 0.54788, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.5479\n",
            "Epoch 63/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5365\n",
            "Epoch 00063: loss improved from 0.54788 to 0.53654, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5365\n",
            "Epoch 64/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5396\n",
            "Epoch 00064: loss did not improve from 0.53654\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5396\n",
            "Epoch 65/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5427\n",
            "Epoch 00065: loss did not improve from 0.53654\n",
            "95/95 [==============================] - 27s 281ms/step - loss: 0.5427\n",
            "Epoch 66/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5330\n",
            "Epoch 00066: loss improved from 0.53654 to 0.53302, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 0.5330\n",
            "Epoch 67/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5202\n",
            "Epoch 00067: loss improved from 0.53302 to 0.52022, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.5202\n",
            "Epoch 68/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5344\n",
            "Epoch 00068: loss did not improve from 0.52022\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.5344\n",
            "Epoch 69/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5278\n",
            "Epoch 00069: loss did not improve from 0.52022\n",
            "95/95 [==============================] - 26s 279ms/step - loss: 0.5278\n",
            "Epoch 70/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5276\n",
            "Epoch 00070: loss did not improve from 0.52022\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5276\n",
            "Epoch 71/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5147\n",
            "Epoch 00071: loss improved from 0.52022 to 0.51468, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5147\n",
            "Epoch 72/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4907\n",
            "Epoch 00072: loss improved from 0.51468 to 0.49074, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4907\n",
            "Epoch 73/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5055\n",
            "Epoch 00073: loss did not improve from 0.49074\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5055\n",
            "Epoch 74/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5080\n",
            "Epoch 00074: loss did not improve from 0.49074\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.5080\n",
            "Epoch 75/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4892\n",
            "Epoch 00075: loss improved from 0.49074 to 0.48918, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4892\n",
            "Epoch 76/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.5017\n",
            "Epoch 00076: loss did not improve from 0.48918\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.5017\n",
            "Epoch 77/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4820\n",
            "Epoch 00077: loss improved from 0.48918 to 0.48204, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4820\n",
            "Epoch 78/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4903\n",
            "Epoch 00078: loss did not improve from 0.48204\n",
            "95/95 [==============================] - 26s 274ms/step - loss: 0.4903\n",
            "Epoch 79/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4710\n",
            "Epoch 00079: loss improved from 0.48204 to 0.47099, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4710\n",
            "Epoch 80/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4667\n",
            "Epoch 00080: loss improved from 0.47099 to 0.46673, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4667\n",
            "Epoch 81/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4760\n",
            "Epoch 00081: loss did not improve from 0.46673\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4760\n",
            "Epoch 82/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4601\n",
            "Epoch 00082: loss improved from 0.46673 to 0.46008, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4601\n",
            "Epoch 83/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4724\n",
            "Epoch 00083: loss did not improve from 0.46008\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4724\n",
            "Epoch 84/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4517\n",
            "Epoch 00084: loss improved from 0.46008 to 0.45170, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4517\n",
            "Epoch 85/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4681\n",
            "Epoch 00085: loss did not improve from 0.45170\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4681\n",
            "Epoch 86/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4461\n",
            "Epoch 00086: loss improved from 0.45170 to 0.44609, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4461\n",
            "Epoch 87/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4455\n",
            "Epoch 00087: loss improved from 0.44609 to 0.44550, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4455\n",
            "Epoch 88/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4328\n",
            "Epoch 00088: loss improved from 0.44550 to 0.43279, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 278ms/step - loss: 0.4328\n",
            "Epoch 89/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4401\n",
            "Epoch 00089: loss did not improve from 0.43279\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4401\n",
            "Epoch 90/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4434\n",
            "Epoch 00090: loss did not improve from 0.43279\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4434\n",
            "Epoch 91/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4320\n",
            "Epoch 00091: loss improved from 0.43279 to 0.43201, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 277ms/step - loss: 0.4320\n",
            "Epoch 92/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4121\n",
            "Epoch 00092: loss improved from 0.43201 to 0.41211, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.4121\n",
            "Epoch 93/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4259\n",
            "Epoch 00093: loss did not improve from 0.41211\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4259\n",
            "Epoch 94/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4225\n",
            "Epoch 00094: loss did not improve from 0.41211\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4225\n",
            "Epoch 95/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4107\n",
            "Epoch 00095: loss improved from 0.41211 to 0.41069, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4107\n",
            "Epoch 96/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4212\n",
            "Epoch 00096: loss did not improve from 0.41069\n",
            "95/95 [==============================] - 26s 275ms/step - loss: 0.4212\n",
            "Epoch 97/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4153\n",
            "Epoch 00097: loss did not improve from 0.41069\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4153\n",
            "Epoch 98/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4048\n",
            "Epoch 00098: loss improved from 0.41069 to 0.40484, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 280ms/step - loss: 0.4048\n",
            "Epoch 99/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.3990\n",
            "Epoch 00099: loss improved from 0.40484 to 0.39898, saving model to weights_E.hdf5\n",
            "95/95 [==============================] - 27s 279ms/step - loss: 0.3990\n",
            "Epoch 100/100\n",
            "95/95 [==============================] - ETA: 0s - loss: 0.4111\n",
            "Epoch 00100: loss did not improve from 0.39898\n",
            "95/95 [==============================] - 26s 276ms/step - loss: 0.4111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: GenerativeModel_compiled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: GenerativeModel_compiled/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff8c6d0b8d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff8c3373fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGS9_KauXaGN"
      },
      "source": [
        "np.random.seed(1337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rk-XuroXaIK"
      },
      "source": [
        "def sample(preds): #Taking sample to make \n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 0.2\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ3dgNXFXaO8",
        "outputId": "b9debeb5-0c7f-43ab-92b1-83dddf193ace"
      },
      "source": [
        "N_CHARS = None\n",
        "\n",
        "def create_index_char_map(corpus, verbose=0):\n",
        "    chars = sorted(list(set(corpus)))\n",
        "    global N_CHARS\n",
        "    N_CHARS = len(chars)\n",
        "    if verbose:\n",
        "        print('No. of unique characters:', N_CHARS)\n",
        "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "    return chars, char_to_idx, idx_to_char\n",
        "\n",
        "chars, char_to_idx, idx_to_char = create_index_char_map(text, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of unique characters: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RDgX-2-LoiU",
        "outputId": "af367004-dfb3-4f03-f4b5-61e0eb31b906"
      },
      "source": [
        "def generate_tweets(model, corpus, char_to_idx, idx_to_char, n_tweets=10, verbose=0): \n",
        "    model.load_weights('weights_E.hdf5')\n",
        "    tweets = []\n",
        "    spaces_in_corpus = np.array([idx for idx in range(len(corpus)) if corpus[idx] == ' '])\n",
        "    for i in range(1, n_tweets + 1):\n",
        "        begin = np.random.choice(spaces_in_corpus)\n",
        "        tweet = u''\n",
        "        sequence = corpus[begin:begin + maxlen]\n",
        "        tweet += sequence\n",
        "        if verbose:\n",
        "            print('Tweet no. %03d' % i)\n",
        "            print('=' * 13)\n",
        "            print('Generating with seed:')\n",
        "            print(sequence)\n",
        "            print('_' * len(sequence))\n",
        "        for _ in range(100):\n",
        "            x = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sequence):\n",
        "                x[0, t, char_to_idx[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_idx = sample(preds)\n",
        "            next_char = idx_to_char[next_idx]\n",
        "\n",
        "            tweet += next_char\n",
        "            sequence = sequence[1:] + next_char\n",
        "        if verbose:\n",
        "            print(tweet)\n",
        "            print()\n",
        "        tweets.append(tweet)\n",
        "    return tweets\n",
        "\n",
        "tweets = generate_tweets(model, text, char_to_idx, idx_to_char, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet no. 001\n",
            "=============\n",
            "Generating with seed:\n",
            " definit infect way nba need space fan p\n",
            "________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " definit infect way nba need space fan ppin prine dfecent covid hospit data\n",
            "303,wonder seen bid well mrder covid rest covid work go holit co\n",
            "\n",
            "Tweet no. 002\n",
            "=============\n",
            "Generating with seed:\n",
            " abl polic thiswhat point mask covid\n",
            "310\n",
            "________________________________________\n",
            " abl polic thiswhat point mask covid\n",
            "310,seen amp light twant seed work provid case covid\n",
            "131,lover bealth alpray best mone treet sone pree \n",
            "\n",
            "Tweet no. 003\n",
            "=============\n",
            "Generating with seed:\n",
            " number posit case area popul multipli k\n",
            "________________________________________\n",
            " number posit case area popul multipli k\n",
            "295,stent antit hope get thank share latest child teen covid hospit data\n",
            "303,wonder seem brier chil\n",
            "\n",
            "Tweet no. 004\n",
            "=============\n",
            "Generating with seed:\n",
            " case work first name appar popular name\n",
            "________________________________________\n",
            " case work first name appar popular name anomit covid\n",
            "191,got vaccin heart like meet place go viner sure covid tuse covid imp bili ack best \n",
            "\n",
            "Tweet no. 005\n",
            "=============\n",
            "Generating with seed:\n",
            " deathlingscovid gunviol\n",
            "103,nice spinco\n",
            "________________________________________\n",
            " deathlingscovid gunviol\n",
            "103,nice spincomil covid went covid restectit omicronvari use amp enter corld day peopl sup treat bact covid wear c\n",
            "\n",
            "Tweet no. 006\n",
            "=============\n",
            "Generating with seed:\n",
            " today freedom taken waitxrp xrpcommun c\n",
            "________________________________________\n",
            " today freedom taken waitxrp xrpcommun covid\n",
            "191,life shind ficialth auster vaccin pandem amp pill nead peopl stay covid wear corld dose pos\n",
            "\n",
            "Tweet no. 007\n",
            "=============\n",
            "Generating with seed:\n",
            " time covid\n",
            "154,iron california want cre\n",
            "________________________________________\n",
            " time covid\n",
            "154,iron california want cread enorg covid\n",
            "191,got none pandem covid put enither covid stay test mone rest covid get state covid\n",
            "\n",
            "Tweet no. 008\n",
            "=============\n",
            "Generating with seed:\n",
            " countrythey report fresh covid case tod\n",
            "________________________________________\n",
            " countrythey report fresh covid case today useend covid rest could see mane mask mandat hope provid case new covid hospit data\n",
            "303,wonder se\n",
            "\n",
            "Tweet no. 009\n",
            "=============\n",
            "Generating with seed:\n",
            " grantcardon kevinhart netflix stockmark\n",
            "________________________________________\n",
            " grantcardon kevinhart netflix stockmark\n",
            "173,covid ffe time amp like souron covid case intreassot novedom safe infect eyork vaccin covid wea\n",
            "\n",
            "Tweet no. 010\n",
            "=============\n",
            "Generating with seed:\n",
            " would support covid uk covid\n",
            "259,realis\n",
            "________________________________________\n",
            " would support covid uk covid\n",
            "259,realist antic provid case tound round covid\n",
            "159,woold covid covidvaccina covid\n",
            "199,go ffect covid wear cor\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-49-wmsAuDN"
      },
      "source": [
        "#Evaluating the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82S2PTmELoki"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import pairwise_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RUap8QqLoms",
        "outputId": "183d1404-67d4-472c-f9b5-5776356a2959"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(sentences)\n",
        "Xval = vectorizer.transform(tweets)\n",
        "print(pairwise_distances(Xval, Y=tfidf, metric='cosine').min(axis=1).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4403126458239652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpINDD9JEdXd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN2WGJZWEdft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8eYXlF2Lous"
      },
      "source": [
        "#References:\n",
        "#https://keras.io/examples/generative/lstm_character_level_text_generation/\n",
        "#https://towardsdatascience.com/tweet-generation-with-neural-networks-lstm-and-gpt-2-e163bfd3fbd8\n",
        "#https://towardsdatascience.com/predicting-trump-tweets-with-a-rnn-95e7c398b18e\n",
        "#https://gilberttanner.com/blog/generating-text-using-a-recurrent-neuralnetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUez4YTYLoyp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}